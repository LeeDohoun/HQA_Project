# íŒŒì¼: src/data_pipeline/crawler.py
"""
ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬ë¡¤ëŸ¬
- ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ëª©ë¡ ìˆ˜ì§‘
- PDF ë‹¤ìš´ë¡œë“œ ë° ë³¸ë¬¸ ì¶”ì¶œ
"""

import os
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class Report:
    """ë¦¬í¬íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str              # ì œëª©
    broker: str             # ì¦ê¶Œì‚¬
    date: str               # ì‘ì„±ì¼
    link: str               # ìƒì„¸í˜ì´ì§€ URL
    pdf_url: Optional[str] = None  # PDF ë‹¤ìš´ë¡œë“œ URL
    stock_code: str = ""    # ì¢…ëª©ì½”ë“œ
    stock_name: str = ""    # ì¢…ëª©ëª…
    
    def to_dict(self) -> Dict:
        return {
            "title": self.title,
            "broker": self.broker,
            "date": self.date,
            "link": self.link,
            "pdf_url": self.pdf_url,
            "stock_code": self.stock_code,
            "stock_name": self.stock_name
        }


class ReportCrawler:
    """ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, download_dir: str = "./data/reports"):
        self.base_url = "https://finance.naver.com/research/company_list.naver"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://finance.naver.com/research/'
        }
        self.download_dir = download_dir
        os.makedirs(download_dir, exist_ok=True)

    def fetch_latest_reports(
        self,
        stock_code: str,
        max_count: int = 5
    ) -> List[Report]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ìµœì‹  ë¦¬í¬íŠ¸ ëª©ë¡ ìˆ˜ì§‘
        
        Args:
            stock_code: ì¢…ëª©ì½”ë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            Report ë¦¬ìŠ¤íŠ¸
        """
        params = {
            'searchType': 'itemCode',
            'itemCode': stock_code,
            'page': 1
        }
        
        try:
            print(f"   ğŸ“¥ {stock_code} ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
            response = requests.get(self.base_url, headers=self.headers, params=params)
            response.encoding = 'euc-kr'
            
            if response.status_code != 200:
                print(f"   âŒ ì„œë²„ ì ‘ì† ì‹¤íŒ¨ (ìƒíƒœì½”ë“œ: {response.status_code})")
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.select_one('table.type_1')
            
            if not table:
                print("   âŒ ë¦¬í¬íŠ¸ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

            reports = []
            rows = table.find_all('tr')
            
            for row in rows:
                cols = row.find_all('td')
                
                if len(cols) >= 5:
                    title_tag = cols[1].find('a')
                    broker = cols[2].text.strip()
                    date = cols[4].text.strip()
                    
                    # PDF ë§í¬ í™•ì¸
                    pdf_tag = cols[3].find('a')
                    pdf_url = None
                    if pdf_tag and pdf_tag.get('href'):
                        pdf_url = pdf_tag.get('href')
                    
                    if title_tag:
                        reports.append(Report(
                            title=title_tag.text.strip(),
                            broker=broker,
                            date=date,
                            link="https://finance.naver.com/research/" + title_tag['href'],
                            pdf_url=pdf_url,
                            stock_code=stock_code
                        ))
                        
                        if len(reports) >= max_count:
                            break
            
            print(f"   âœ… {len(reports)}ê°œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
            return reports

        except Exception as e:
            print(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def download_pdf(self, report: Report) -> Optional[str]:
        """
        ë¦¬í¬íŠ¸ PDF ë‹¤ìš´ë¡œë“œ
        
        Args:
            report: Report ê°ì²´
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        if not report.pdf_url:
            print(f"   âš ï¸ PDF URL ì—†ìŒ: {report.title}")
            return None
        
        try:
            response = requests.get(report.pdf_url, headers=self.headers)
            
            if response.status_code == 200:
                # íŒŒì¼ëª… ìƒì„±
                safe_title = "".join(c for c in report.title if c.isalnum() or c in (' ', '-', '_'))[:50]
                filename = f"{report.date}_{report.broker}_{safe_title}.pdf"
                filepath = os.path.join(self.download_dir, filename)
                
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                print(f"   ğŸ’¾ PDF ì €ì¥: {filename}")
                return filepath
            else:
                print(f"   âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"   âŒ PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def fetch_and_download(
        self,
        stock_code: str,
        max_count: int = 3
    ) -> List[Dict]:
        """
        ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° PDF ë‹¤ìš´ë¡œë“œ í†µí•©
        
        Args:
            stock_code: ì¢…ëª©ì½”ë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            ë¦¬í¬íŠ¸ ì •ë³´ + íŒŒì¼ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        reports = self.fetch_latest_reports(stock_code, max_count)
        results = []
        
        for report in reports:
            filepath = self.download_pdf(report)
            result = report.to_dict()
            result['local_path'] = filepath
            results.append(result)
        
        return results


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    crawler = ReportCrawler()
    reports = crawler.fetch_latest_reports("005930")  # ì‚¼ì„±ì „ì
    for r in reports:
        print(f"- {r.title} ({r.broker}, {r.date})")

    print("--- ì‚¼ì„±ì „ì(005930) ---")
    res = crawler.fetch_latest_reports("005930")
    print(res)