# íŒŒì¼: src/data_pipeline/news_crawler.py
"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- ë„¤ì´ë²„ ë‰´ìŠ¤, êµ¬ê¸€ ë‰´ìŠ¤ ë“±ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
- í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
from dataclasses import dataclass


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str              # ì œëª©
    summary: str            # ìš”ì•½/ë³¸ë¬¸ ì¼ë¶€
    source: str             # ì–¸ë¡ ì‚¬
    url: str                # ê¸°ì‚¬ URL
    published_at: str       # ë°œí–‰ì¼
    keyword: str            # ê²€ìƒ‰ í‚¤ì›Œë“œ
    
    def to_dict(self) -> Dict:
        return {
            "title": self.title,
            "summary": self.summary,
            "source": self.source,
            "url": self.url,
            "published_at": self.published_at,
            "keyword": self.keyword
        }


class NewsCrawler:
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def fetch_naver_news(
        self,
        keyword: str,
        max_count: int = 10
    ) -> List[NewsArticle]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¢…ëª©ëª… ë“±)
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            NewsArticle ë¦¬ìŠ¤íŠ¸
        """
        url = "https://search.naver.com/search.naver"
        params = {
            "where": "news",
            "query": keyword,
            "sort": "1"  # ìµœì‹ ìˆœ
        }
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            articles = []
            news_items = soup.select('div.news_area')[:max_count]
            
            for item in news_items:
                title_tag = item.select_one('a.news_tit')
                desc_tag = item.select_one('div.news_dsc')
                source_tag = item.select_one('a.info.press')
                date_tag = item.select_one('span.info')
                
                if title_tag:
                    articles.append(NewsArticle(
                        title=title_tag.get('title', title_tag.text.strip()),
                        summary=desc_tag.text.strip() if desc_tag else "",
                        source=source_tag.text.strip() if source_tag else "Unknown",
                        url=title_tag.get('href', ''),
                        published_at=date_tag.text.strip() if date_tag else "",
                        keyword=keyword
                    ))
            
            print(f"ğŸ“° '{keyword}' ë‰´ìŠ¤ {len(articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_stock_news(
        self,
        stock_code: str,
        stock_name: str,
        max_count: int = 10
    ) -> List[NewsArticle]:
        """
        ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
        
        Args:
            stock_code: ì¢…ëª©ì½”ë“œ
            stock_name: ì¢…ëª©ëª…
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            NewsArticle ë¦¬ìŠ¤íŠ¸
        """
        # ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
        return self.fetch_naver_news(stock_name, max_count)
    
    def fetch_article_content(self, url: str) -> Optional[str]:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
        
        Args:
            url: ê¸°ì‚¬ URL
            
        Returns:
            ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            response = requests.get(url, headers=self.headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (articleBody)
            content = soup.select_one('article#dic_area')
            if content:
                return content.get_text(strip=True)
            
            # ì¼ë°˜ì ì¸ ë³¸ë¬¸ íƒœê·¸ ì‹œë„
            for selector in ['article', '.article_body', '.news_content', '#content']:
                content = soup.select_one(selector)
                if content:
                    return content.get_text(strip=True)
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    crawler = NewsCrawler()
    articles = crawler.fetch_naver_news("ì‚¼ì„±ì „ì", max_count=5)
    for article in articles:
        print(f"- {article.title} ({article.source})")
