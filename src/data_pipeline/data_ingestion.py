# íŒŒì¼: src/data_pipeline/data_ingestion.py
"""
ë°ì´í„° ìˆ˜ì§‘ â†’ ì›ë³¸ ì €ì¥ â†’ RAG ë²¡í„°í™” í†µí•© íŒŒì´í”„ë¼ì¸

ì•„í‚¤í…ì²˜ (v0.3.0):
- OCR: PaddleOCR-VL-1.5 (í…ìŠ¤íŠ¸ ì „ìš© ë³€í™˜)
- Embedding: Snowflake Arctic Korean (1024 dim)
- Reranker: Qwen3-Reranker-0.6B
- Vector DB: ChromaDB

íë¦„:
1. ë°ì´í„° ìˆ˜ì§‘ (í¬ë¡¤ëŸ¬, API)
2. ì›ë³¸ DB ì €ì¥ (SQLite) - ì¤‘ë³µ ì²´í¬
3. RAG ë²¡í„°í™” (ChromaDB) - ë¯¸ì²˜ë¦¬ ë°ì´í„°ë§Œ
"""

import os
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import logging

# ë°ì´í„° ìˆ˜ì§‘ê¸°
from .price_loader import PriceLoader
from .dart_collector import DARTCollector, Disclosure
from .news_crawler import NewsCrawler, NewsArticle
from .crawler import ReportCrawler, Report

# ì›ë³¸ ë°ì´í„° ì €ì¥ì†Œ
from src.database.raw_data_store import (
    RawDataStore,
    RawReport,
    RawNews,
    RawDisclosure,
    RawPriceData
)

# RAG ëª¨ë“ˆ (í…ìŠ¤íŠ¸ ì „ìš©)
from src.rag import RAGRetriever

logger = logging.getLogger(__name__)


class DataIngestionPipeline:
    """
    ë°ì´í„° ìˆ˜ì§‘ â†’ ì›ë³¸ ì €ì¥ â†’ RAG ë²¡í„°í™” í†µí•© íŒŒì´í”„ë¼ì¸
    
    ì•„í‚¤í…ì²˜:
    - ëª¨ë“  ë¬¸ì„œëŠ” PaddleOCR-VL-1.5ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
    - Snowflake Arctic Korean ì„ë² ë”©
    - Qwen3-Rerankerë¡œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹
    """
    
    def __init__(
        self,
        db_path: str = "./database/raw_data.db",
        files_dir: str = "./data/files",
        vector_persist_dir: str = "./database/chroma_db",
        collection_name: str = "stock_data",
        embedding_type: str = "default",  # Snowflake Arctic Korean
        # ë¦¬ë­ì»¤ ì„¤ì •
        use_reranker: bool = True,
        retrieval_k: int = 20,
        rerank_top_k: int = 3
    ):
        """
        Args:
            db_path: SQLite DB ê²½ë¡œ
            files_dir: PDF ë“± íŒŒì¼ ì €ì¥ ê²½ë¡œ
            vector_persist_dir: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
            collection_name: ë²¡í„° ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_type: ì„ë² ë”© ëª¨ë¸ íƒ€ì…
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
            retrieval_k: ë²¡í„° ê²€ìƒ‰ í›„ë³´ ìˆ˜
            rerank_top_k: ë¦¬ë­í‚¹ í›„ ìµœì¢… ë°˜í™˜ ìˆ˜
        """
        print("\n" + "="*60)
        print("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        print("="*60)
        
        # 1. ë°ì´í„° ìˆ˜ì§‘ê¸°
        print("\nğŸ“¦ [1/4] ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”...")
        self.price_loader = PriceLoader()
        self.dart_collector = DARTCollector()
        self.news_crawler = NewsCrawler()
        self.report_crawler = ReportCrawler(download_dir=os.path.join(files_dir, "reports"))
        print("   âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. ì›ë³¸ ë°ì´í„° ì €ì¥ì†Œ (SQLite)
        print("\nğŸ’¾ [2/4] ì›ë³¸ ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™”...")
        self.raw_store = RawDataStore(db_path=db_path, files_dir=files_dir)
        print(f"   âœ… SQLite DB: {db_path}")
        
        # 3. RAG ê²€ìƒ‰ê¸° (PaddleOCR + Snowflake Arctic + Qwen3 Reranker)
        print("\nğŸ§  [3/4] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...")
        self.retriever = RAGRetriever(
            persist_dir=vector_persist_dir,
            collection_name=collection_name,
            embedding_type=embedding_type,
            use_reranker=use_reranker,
            retrieval_k=retrieval_k,
            rerank_top_k=rerank_top_k,
            reranker_task_type="finance"
        )
        print(f"   âœ… ë²¡í„° DB: {vector_persist_dir}")
        print(f"   âœ… ë¦¬ë­ì»¤: {'í™œì„±í™”' if use_reranker else 'ë¹„í™œì„±í™”'}")
        
        # 4. ì„¤ì • ì €ì¥
        print("\nâš™ï¸ [4/4] ì„¤ì • ì €ì¥...")
        self.files_dir = files_dir
        self.embedding_type = embedding_type
        
        print("\n" + "="*60)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        print("   - OCR: PaddleOCR-VL-1.5")
        print("   - Embedding: Snowflake Arctic Korean")
        print(f"   - Reranker: Qwen3-Reranker-0.6B ({'ON' if use_reranker else 'OFF'})")
        print("="*60 + "\n")
    
    # ==================== ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ ====================
    
    def ingest_stock_data(
        self,
        stock_code: str,
        stock_name: str,
        include_reports: bool = True,
        include_news: bool = True,
        include_dart: bool = True,
        include_price: bool = True,
        auto_embed: bool = True
    ) -> Dict:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ â†’ ì›ë³¸ ì €ì¥ â†’ RAG ë²¡í„°í™”
        
        Args:
            stock_code: ì¢…ëª©ì½”ë“œ
            stock_name: ì¢…ëª©ëª…
            include_reports: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬í•¨ ì—¬ë¶€
            include_news: ë‰´ìŠ¤ í¬í•¨ ì—¬ë¶€
            include_dart: DART ê³µì‹œ í¬í•¨ ì—¬ë¶€
            include_price: ì£¼ê°€ ë°ì´í„° í¬í•¨ ì—¬ë¶€
            auto_embed: ìˆ˜ì§‘ í›„ ìë™ìœ¼ë¡œ RAG ì„ë² ë”© ì—¬ë¶€
            
        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š [{stock_name}({stock_code})] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*60}")
        
        results = {
            "stock_code": stock_code,
            "stock_name": stock_name,
            "timestamp": datetime.now().isoformat(),
            "collected": {"reports": 0, "news": 0, "disclosures": 0, "price": 0},
            "new_items": {"reports": 0, "news": 0, "disclosures": 0},
            "embedded": {"reports": 0, "news": 0, "disclosures": 0}
        }
        
        # 1. ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
        if include_price:
            results["collected"]["price"] = self._collect_price(stock_code, stock_name)
        
        # 2. ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° ì €ì¥
        if include_reports:
            collected, new = self._collect_reports(stock_code, stock_name)
            results["collected"]["reports"] = collected
            results["new_items"]["reports"] = new
        
        # 3. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥
        if include_news:
            collected, new = self._collect_news(stock_code, stock_name)
            results["collected"]["news"] = collected
            results["new_items"]["news"] = new
        
        # 4. DART ê³µì‹œ ìˆ˜ì§‘ ë° ì €ì¥
        if include_dart:
            collected, new = self._collect_disclosures(stock_code, stock_name)
            results["collected"]["disclosures"] = collected
            results["new_items"]["disclosures"] = new
        
        # 5. ë¯¸ì„ë² ë”© ë°ì´í„° â†’ RAG ë²¡í„°í™”
        if auto_embed:
            print(f"\nğŸ”„ RAG ë²¡í„°í™” ì‹œì‘...")
            embedded = self.embed_pending_data(stock_code)
            results["embedded"] = embedded
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary(results)
        
        return results
    
    # ==================== ê°œë³„ ìˆ˜ì§‘ í•¨ìˆ˜ ====================
    
    def _collect_price(self, stock_code: str, stock_name: str) -> int:
        """ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
        print(f"\nğŸ“ˆ [1/4] ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘...")
        
        try:
            df = self.price_loader.get_stock_data(stock_code, days=300)
            
            if len(df) < 150:
                print(f"   âš ï¸ ë°ì´í„° ë¶€ì¡± ({len(df)}ì¼)")
                return 0
            
            # 150ì¼ ì´í‰ì„  ê³„ì‚°
            df['MA150'] = df['Close'].rolling(window=150).mean()
            
            # ìµœê·¼ ë°ì´í„°ë§Œ ì €ì¥ (ìµœê·¼ 30ì¼)
            recent_df = df.tail(30)
            count = 0
            
            for date, row in recent_df.iterrows():
                is_bullish = row['Close'] > row['MA150'] if row['MA150'] else None
                
                price_data = RawPriceData(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    date=date.strftime("%Y-%m-%d"),
                    open_price=row['Open'],
                    high_price=row['High'],
                    low_price=row['Low'],
                    close_price=row['Close'],
                    volume=int(row['Volume']),
                    ma150=row['MA150'] if row['MA150'] else None,
                    is_bullish=is_bullish
                )
                self.raw_store.save_price_data(price_data)
                count += 1
            
            print(f"   âœ… {count}ì¼ì¹˜ ì£¼ê°€ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            return count
            
        except Exception as e:
            print(f"   âŒ ì£¼ê°€ ë°ì´í„° ì˜¤ë¥˜: {e}")
            logger.exception("ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜")
            return 0
    
    def _collect_reports(self, stock_code: str, stock_name: str) -> Tuple[int, int]:
        """ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ë° ì €ì¥"""
        print(f"\nğŸ“‘ [2/4] ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘...")
        
        try:
            reports = self.report_crawler.fetch_and_download(stock_code, max_count=5)
            new_count = 0
            
            for report in reports:
                # ì¤‘ë³µ ì²´í¬
                if self.raw_store.is_report_exists(report['link']):
                    continue
                
                # ì›ë³¸ ì €ì¥
                raw_report = RawReport(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    title=report['title'],
                    broker=report['broker'],
                    report_date=report['date'],
                    link=report['link'],
                    pdf_path=report.get('local_path')
                )
                self.raw_store.save_report(raw_report)
                new_count += 1
            
            print(f"   âœ… {len(reports)}ê°œ ìˆ˜ì§‘, {new_count}ê°œ ì‹ ê·œ ì €ì¥")
            return len(reports), new_count
            
        except Exception as e:
            print(f"   âŒ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            logger.exception("ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜")
            return 0, 0
    
    def _collect_news(self, stock_code: str, stock_name: str) -> Tuple[int, int]:
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥"""
        print(f"\nğŸ“° [3/4] ë‰´ìŠ¤ ìˆ˜ì§‘...")
        
        try:
            articles = self.news_crawler.fetch_stock_news(stock_code, stock_name, max_count=10)
            new_count = 0
            
            for article in articles:
                # ì¤‘ë³µ ì²´í¬
                if self.raw_store.is_news_exists(article.url):
                    continue
                
                # ì›ë³¸ ì €ì¥
                raw_news = RawNews(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    title=article.title,
                    summary=article.summary,
                    source=article.source,
                    url=article.url,
                    published_at=article.published_at
                )
                self.raw_store.save_news(raw_news)
                new_count += 1
            
            print(f"   âœ… {len(articles)}ê°œ ìˆ˜ì§‘, {new_count}ê°œ ì‹ ê·œ ì €ì¥")
            return len(articles), new_count
            
        except Exception as e:
            print(f"   âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            logger.exception("ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜")
            return 0, 0
    
    def _collect_disclosures(self, stock_code: str, stock_name: str) -> Tuple[int, int]:
        """DART ê³µì‹œ ìˆ˜ì§‘ ë° ì €ì¥"""
        print(f"\nğŸ“‹ [4/4] DART ê³µì‹œ ìˆ˜ì§‘...")
        
        if not self.dart_collector.api_key:
            print("   âš ï¸ DART API í‚¤ ë¯¸ì„¤ì • - ê±´ë„ˆëœ€")
            return 0, 0
        
        try:
            disclosures = self.dart_collector.fetch_disclosures(
                stock_code=stock_code,
                max_count=10
            )
            new_count = 0
            
            for disc in disclosures:
                # ì¤‘ë³µ ì²´í¬ (receipt_no ê¸°ì¤€)
                existing = self.raw_store.get_disclosures(stock_code)
                if any(d.receipt_no == disc.rcept_no for d in existing):
                    continue
                
                # ì›ë³¸ ì €ì¥
                raw_disc = RawDisclosure(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    corp_code=disc.corp_code,
                    report_name=disc.report_nm,
                    receipt_no=disc.rcept_no,
                    receipt_date=disc.rcept_dt,
                    submitter=disc.flr_nm,
                    url=disc.url
                )
                self.raw_store.save_disclosure(raw_disc)
                new_count += 1
            
            print(f"   âœ… {len(disclosures)}ê°œ ìˆ˜ì§‘, {new_count}ê°œ ì‹ ê·œ ì €ì¥")
            return len(disclosures), new_count
            
        except Exception as e:
            print(f"   âŒ ê³µì‹œ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            logger.exception("ê³µì‹œ ìˆ˜ì§‘ ì˜¤ë¥˜")
            return 0, 0
    
    # ==================== RAG ì„ë² ë”© ====================
    
    def embed_pending_data(self, stock_code: Optional[str] = None) -> Dict:
        """
        ë¯¸ì„ë² ë”© ë°ì´í„°ë¥¼ RAG ë²¡í„°í™”
        
        ì²˜ë¦¬ ë°©ì‹ (PaddleOCR-VL + Snowflake Arctic):
        - ë¦¬í¬íŠ¸ PDF: PaddleOCR-VLë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ Snowflake Arctic ì„ë² ë”©
        - ë‰´ìŠ¤/ê³µì‹œ: Snowflake Arctic í…ìŠ¤íŠ¸ ì„ë² ë”©
        - ì£¼ê°€ ë°ì´í„°: ì„ë² ë”© ì œì™¸ (êµ¬ì¡°í™” ë°ì´í„°ë¡œ ë³„ë„ ë¶„ì„)
        
        Args:
            stock_code: íŠ¹ì • ì¢…ëª©ë§Œ ì²˜ë¦¬ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ì„ë² ë”© ê²°ê³¼
        """
        results = {"reports": 0, "news": 0, "disclosures": 0}
        
        print(f"\n{'='*50}")
        print(f"ğŸ§  PaddleOCR-VL + Snowflake Arctic ì„ë² ë”© ì²˜ë¦¬")
        print(f"{'='*50}")
        
        # 1. ë¯¸ì„ë² ë”© ë¦¬í¬íŠ¸ ì²˜ë¦¬ (PDF â†’ OCR â†’ ì„ë² ë”©)
        reports = self.raw_store.get_reports(stock_code, not_embedded_only=True)
        print(f"\nğŸ“‘ [1/3] ë¦¬í¬íŠ¸ ì²˜ë¦¬ ({len(reports)}ê±´)")
        
        for report in reports:
            try:
                metadata = {
                    "stock_code": report.stock_code,
                    "stock_name": report.stock_name,
                    "data_type": "report",
                    "broker": report.broker,
                    "report_date": report.report_date,
                    "source_id": report.id,
                    "source_url": report.link
                }
                
                # PDF â†’ PaddleOCR-VL â†’ í…ìŠ¤íŠ¸ ì„ë² ë”©
                if report.pdf_path and os.path.exists(report.pdf_path):
                    print(f"   ğŸ“„ OCR ì²˜ë¦¬ ì¤‘: {report.title[:40]}...")
                    
                    # RAGRetrieverì˜ index_document ì‚¬ìš© (ë‚´ë¶€ì—ì„œ OCR ì²˜ë¦¬)
                    result = self.retriever.index_document(
                        file_path=report.pdf_path,
                        metadata=metadata,
                        chunk_text=True
                    )
                    
                    if result.get("success"):
                        self.raw_store.mark_as_embedded("reports", [report.id])
                        results["reports"] += 1
                        print(f"      âœ… {result.get('chunks_added', 0)}ê°œ ì²­í¬ ì„ë² ë”©")
                else:
                    # PDF ì—†ìœ¼ë©´ ë©”íƒ€ì •ë³´ë§Œ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
                    print(f"   ğŸ“ ë©”íƒ€ì •ë³´ë§Œ ì €ì¥: {report.title[:40]}...")
                    text = f"[ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸]\nì œëª©: {report.title}\nì¦ê¶Œì‚¬: {report.broker}\në‚ ì§œ: {report.report_date}"
                    
                    result = self.retriever.index_text(
                        text=text,
                        metadata=metadata
                    )
                    
                    if result.get("success"):
                        self.raw_store.mark_as_embedded("reports", [report.id])
                        results["reports"] += 1
                        
            except Exception as e:
                print(f"   âš ï¸ ë¦¬í¬íŠ¸ ì„ë² ë”© ì‹¤íŒ¨ ({report.title[:30]}): {e}")
                logger.exception(f"ë¦¬í¬íŠ¸ ì„ë² ë”© ì˜¤ë¥˜: {report.id}")
        
        # 2. ë¯¸ì„ë² ë”© ë‰´ìŠ¤ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ â†’ ì„ë² ë”©)
        news_list = self.raw_store.get_news(stock_code, not_embedded_only=True)
        print(f"\nğŸ“° [2/3] ë‰´ìŠ¤ ì²˜ë¦¬ ({len(news_list)}ê±´)")
        
        news_ids = []
        for news in news_list:
            try:
                metadata = {
                    "stock_code": news.stock_code,
                    "stock_name": news.stock_name,
                    "data_type": "news",
                    "source": news.source,
                    "url": news.url,
                    "published_at": news.published_at,
                    "source_id": news.id
                }
                
                # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ êµ¬ì„±
                text = f"[ë‰´ìŠ¤] {news.title}\nì¶œì²˜: {news.source}\n\n{news.summary}"
                if news.content:
                    text += f"\n\n{news.content}"
                
                result = self.retriever.index_text(
                    text=text,
                    metadata=metadata
                )
                
                if result.get("success"):
                    news_ids.append(news.id)
                    
            except Exception as e:
                print(f"   âš ï¸ ë‰´ìŠ¤ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                logger.exception(f"ë‰´ìŠ¤ ì„ë² ë”© ì˜¤ë¥˜: {news.id}")
        
        if news_ids:
            self.raw_store.mark_as_embedded("news", news_ids)
            results["news"] = len(news_ids)
            print(f"   âœ… {len(news_ids)}ê±´ ì„ë² ë”© ì™„ë£Œ")
        
        # 3. ë¯¸ì„ë² ë”© ê³µì‹œ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ â†’ ì„ë² ë”©)
        disclosures = self.raw_store.get_disclosures(stock_code, not_embedded_only=True)
        print(f"\nğŸ“‹ [3/3] ê³µì‹œ ì²˜ë¦¬ ({len(disclosures)}ê±´)")
        
        disc_ids = []
        for disc in disclosures:
            try:
                metadata = {
                    "stock_code": disc.stock_code,
                    "stock_name": disc.stock_name,
                    "data_type": "disclosure",
                    "report_name": disc.report_name,
                    "receipt_date": disc.receipt_date,
                    "url": disc.url,
                    "source_id": disc.id
                }
                
                # ê³µì‹œ í…ìŠ¤íŠ¸ êµ¬ì„±
                text = f"[DART ê³µì‹œ] {disc.report_name}\nì œì¶œì: {disc.submitter}\nì ‘ìˆ˜ì¼: {disc.receipt_date}"
                if disc.content:
                    text += f"\n\n{disc.content}"
                
                result = self.retriever.index_text(
                    text=text,
                    metadata=metadata
                )
                
                if result.get("success"):
                    disc_ids.append(disc.id)
                    
            except Exception as e:
                print(f"   âš ï¸ ê³µì‹œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                logger.exception(f"ê³µì‹œ ì„ë² ë”© ì˜¤ë¥˜: {disc.id}")
        
        if disc_ids:
            self.raw_store.mark_as_embedded("disclosures", disc_ids)
            results["disclosures"] = len(disc_ids)
            print(f"   âœ… {len(disc_ids)}ê±´ ì„ë² ë”© ì™„ë£Œ")
        
        # ì£¼ê°€ ë°ì´í„°ëŠ” ì„ë² ë”© ì œì™¸ (êµ¬ì¡°í™” ë°ì´í„°ë¡œ ë³„ë„ ë¶„ì„)
        print(f"\nğŸ’° ì£¼ê°€ ë°ì´í„°: ì„ë² ë”© ì œì™¸ (SQLiteì—ì„œ ì§ì ‘ ì¡°íšŒ)")
        
        print(f"\n{'='*50}")
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ")
        print(f"   - ë¦¬í¬íŠ¸: {results['reports']}ê±´ (PDFâ†’OCRâ†’í…ìŠ¤íŠ¸)")
        print(f"   - ë‰´ìŠ¤: {results['news']}ê±´")
        print(f"   - ê³µì‹œ: {results['disclosures']}ê±´")
        print(f"{'='*50}")
        
        return results
    
    # ==================== ì§ì ‘ ì¸ë±ì‹± ====================
    
    def index_pdf(
        self,
        file_path: str,
        metadata: Optional[Dict] = None,
        chunk_text: bool = True
    ) -> Dict:
        """
        PDF íŒŒì¼ ì§ì ‘ ì¸ë±ì‹± (SQLite ì €ì¥ ì—†ì´)
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            chunk_text: í…ìŠ¤íŠ¸ ì²­í‚¹ ì—¬ë¶€
            
        Returns:
            ì¸ë±ì‹± ê²°ê³¼
        """
        print(f"\nğŸ“„ PDF ì§ì ‘ ì¸ë±ì‹±: {os.path.basename(file_path)}")
        
        result = self.retriever.index_document(
            file_path=file_path,
            metadata=metadata,
            chunk_text=chunk_text
        )
        
        if result.get("success"):
            print(f"   âœ… {result.get('chunks_added', 0)}ê°œ ì²­í¬ ì¸ë±ì‹± ì™„ë£Œ")
        else:
            print(f"   âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
        
        return result
    
    def index_directory(
        self,
        directory: str,
        file_patterns: List[str] = None,
        metadata: Optional[Dict] = None,
        recursive: bool = True
    ) -> Dict:
        """
        ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ì¼ê´„ ì¸ë±ì‹±
        
        Args:
            directory: ë””ë ‰í† ë¦¬ ê²½ë¡œ
            file_patterns: íŒŒì¼ íŒ¨í„´ (ì˜ˆ: ["*.pdf", "*.txt"])
            metadata: ê³µí†µ ë©”íƒ€ë°ì´í„°
            recursive: í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì¸ë±ì‹± ê²°ê³¼
        """
        import glob
        
        if file_patterns is None:
            file_patterns = ["*.pdf"]
        
        print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ì¸ë±ì‹±: {directory}")
        
        results = {
            "total_files": 0,
            "success": 0,
            "failed": 0,
            "total_chunks": 0
        }
        
        for pattern in file_patterns:
            if recursive:
                search_pattern = os.path.join(directory, "**", pattern)
                files = glob.glob(search_pattern, recursive=True)
            else:
                search_pattern = os.path.join(directory, pattern)
                files = glob.glob(search_pattern)
            
            results["total_files"] += len(files)
            
            for file_path in files:
                file_metadata = {
                    **(metadata or {}),
                    "source_directory": directory,
                    "filename": os.path.basename(file_path)
                }
                
                try:
                    result = self.retriever.index_document(
                        file_path=file_path,
                        metadata=file_metadata,
                        chunk_text=True
                    )
                    
                    if result.get("success"):
                        results["success"] += 1
                        results["total_chunks"] += result.get("chunks_added", 0)
                        print(f"   âœ… {os.path.basename(file_path)}")
                    else:
                        results["failed"] += 1
                        print(f"   âŒ {os.path.basename(file_path)}")
                        
                except Exception as e:
                    results["failed"] += 1
                    print(f"   âŒ {os.path.basename(file_path)}: {e}")
        
        print(f"\nğŸ“Š ì¸ë±ì‹± ê²°ê³¼:")
        print(f"   - ì „ì²´: {results['total_files']}ê°œ")
        print(f"   - ì„±ê³µ: {results['success']}ê°œ")
        print(f"   - ì‹¤íŒ¨: {results['failed']}ê°œ")
        print(f"   - ì²­í¬: {results['total_chunks']}ê°œ")
        
        return results
    
    # ==================== ê²€ìƒ‰ ====================
    
    def search(
        self,
        query: str,
        k: int = 5,
        use_reranker: bool = True,
        filter_metadata: Optional[Dict] = None
    ) -> List:
        """
        RAG ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
            filter_metadata: ë©”íƒ€ë°ì´í„° í•„í„°
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        result = self.retriever.retrieve(
            query=query,
            k=k,
            use_reranker=use_reranker
        )
        
        return result.text_results
    
    # ==================== ìœ í‹¸ë¦¬í‹° ====================
    
    def get_stats(self) -> Dict:
        """ì „ì²´ í†µê³„"""
        raw_stats = self.raw_store.get_stats()
        vector_stats = self.retriever.get_stats()
        
        return {
            "raw_data": raw_stats,
            "vector_store": vector_stats
        }
    
    def _print_summary(self, results: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š [{results['stock_name']}] ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ğŸ“¥ ìˆ˜ì§‘: ë¦¬í¬íŠ¸ {results['collected']['reports']}, "
              f"ë‰´ìŠ¤ {results['collected']['news']}, "
              f"ê³µì‹œ {results['collected']['disclosures']}, "
              f"ì£¼ê°€ {results['collected']['price']}ì¼")
        print(f"ğŸ†• ì‹ ê·œ: ë¦¬í¬íŠ¸ {results['new_items']['reports']}, "
              f"ë‰´ìŠ¤ {results['new_items']['news']}, "
              f"ê³µì‹œ {results['new_items']['disclosures']}")
        print(f"ğŸ”— RAG: ë¦¬í¬íŠ¸ {results['embedded']['reports']}, "
              f"ë‰´ìŠ¤ {results['embedded']['news']}, "
              f"ê³µì‹œ {results['embedded']['disclosures']}")
        print(f"{'='*60}")
    
    def clear_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ì£¼ì˜!)"""
        print("âš ï¸ ë²¡í„° ì €ì¥ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        self.retriever.vector_store.clear()
        print("âœ… ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def rebuild_embeddings(self, stock_code: Optional[str] = None):
        """
        ì„ë² ë”© ì¬êµ¬ì¶• (ëª¨ë“  ë°ì´í„° ë‹¤ì‹œ ì„ë² ë”©)
        
        Args:
            stock_code: íŠ¹ì • ì¢…ëª©ë§Œ ì¬êµ¬ì¶• (Noneì´ë©´ ì „ì²´)
        """
        print("ğŸ”„ ì„ë² ë”© ì¬êµ¬ì¶• ì‹œì‘...")
        
        # is_embedded í”Œë˜ê·¸ ì´ˆê¸°í™”
        self.raw_store.reset_embedded_flags(stock_code)
        
        # ë‹¤ì‹œ ì„ë² ë”©
        results = self.embed_pending_data(stock_code)
        
        print(f"âœ… ì¬êµ¬ì¶• ì™„ë£Œ: {results}")
        return results


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = DataIngestionPipeline(
        use_reranker=True,
        retrieval_k=20,
        rerank_top_k=3
    )
    
    # ì‚¼ì„±ì „ì ë°ì´í„° ìˆ˜ì§‘
    results = pipeline.ingest_stock_data(
        stock_code="005930",
        stock_name="ì‚¼ì„±ì „ì",
        include_dart=False  # API í‚¤ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
    )
    
    # í†µê³„ í™•ì¸
    print("\nğŸ“Š ì €ì¥ì†Œ í†µê³„:")
    stats = pipeline.get_stats()
    print(f"   ì›ë³¸ DB: {stats['raw_data'].get('db_size_mb', 'N/A')}MB")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'ì‚¼ì„±ì „ì ì‹¤ì '")
    docs = pipeline.search("ì‚¼ì„±ì „ì ì‹¤ì ", k=3)
    for doc in docs:
        print(f"- {doc.page_content[:100]}...")
