# íŒŒì¼: src/rag/vector_store.py
"""
ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ëª¨ë“ˆ (í…ìŠ¤íŠ¸ ì „ìš©)
- ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥
- PaddleOCR-VLì´ ëª¨ë“  ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ ì €ì¥
"""

import os
from typing import List, Dict, Optional

from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

from .embeddings import EmbeddingManager
from .document_loader import ProcessedDocument
from .text_splitter import TextSplitter


class VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ í´ë˜ìŠ¤ (í…ìŠ¤íŠ¸ ì „ìš©)"""
    
    def __init__(
        self,
        persist_dir: str = "./database/chroma_db",
        collection_name: str = "documents",
        embedding_type: str = "default",
        image_storage_dir: str = None,  # ë ˆê±°ì‹œ í˜¸í™˜ì„±ìš© (ë¬´ì‹œë¨)
        use_multimodal: bool = False,   # ë ˆê±°ì‹œ í˜¸í™˜ì„±ìš© (ë¬´ì‹œë¨)
        device: Optional[str] = None    # ë ˆê±°ì‹œ í˜¸í™˜ì„±ìš© (ë¬´ì‹œë¨)
    ):
        """
        Args:
            persist_dir: ChromaDB ì €ì¥ ê²½ë¡œ
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_type: ì„ë² ë”© ëª¨ë¸ íƒ€ì… ("default", "korean", "multilingual", "large")
        """
        print("âš™ï¸ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì„ë² ë”© ê´€ë¦¬ì ì´ˆê¸°í™”
        self.embedding_manager = EmbeddingManager(model_type=embedding_type)
        self.embeddings = self.embedding_manager.get_langchain_embeddings()
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = TextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # ì €ì¥ì†Œ ì„¤ì •
        self.persist_dir = persist_dir
        os.makedirs(persist_dir, exist_ok=True)
        
        # í…ìŠ¤íŠ¸ìš© ë²¡í„° ì €ì¥ì†Œ
        self.text_store = Chroma(
            collection_name=f"{collection_name}_text",
            embedding_function=self.embeddings,
            persist_directory=persist_dir
        )
        
        print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ (ê²½ë¡œ: {persist_dir})")
    
    def add_document(
        self,
        processed_doc: ProcessedDocument,
        doc_metadata: Optional[Dict] = None,
        chunk_text: bool = True
    ) -> Dict:
        """
        ì²˜ë¦¬ëœ ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
        
        Args:
            processed_doc: ProcessedDocument ê°ì²´
            doc_metadata: ë¬¸ì„œ ì „ì²´ì— ì ìš©í•  ë©”íƒ€ë°ì´í„°
            chunk_text: í…ìŠ¤íŠ¸ ì²­í‚¹ ì—¬ë¶€
            
        Returns:
            ì €ì¥ ê²°ê³¼ ìš”ì•½
        """
        if doc_metadata is None:
            doc_metadata = {}
        
        text_docs = []
        
        for page in processed_doc.pages:
            base_metadata = {
                "source": processed_doc.source,
                "page_num": page.page_num,
                "content_type": page.content_type,
                **doc_metadata,
                **page.metadata
            }
            
            # ëª¨ë“  í˜ì´ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ (PaddleOCRê°€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•¨)
            content = page.content or page.text_fallback
            if content and content.strip():
                if chunk_text:
                    # í…ìŠ¤íŠ¸ ì²­í‚¹
                    chunks = self.text_splitter.split_text(
                        content, 
                        metadata=base_metadata
                    )
                    for chunk in chunks:
                        doc = Document(
                            page_content=chunk.content,
                            metadata={**base_metadata, **chunk.metadata}
                        )
                        text_docs.append(doc)
                else:
                    # ì²­í‚¹ ì—†ì´ ì „ì²´ ì €ì¥
                    doc = Document(
                        page_content=content,
                        metadata=base_metadata
                    )
                    text_docs.append(doc)
        
        # ì €ì¥
        if text_docs:
            self.text_store.add_documents(text_docs)
            print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ì²­í¬ {len(text_docs)}ê°œ ì €ì¥ ì™„ë£Œ")
        
        return {
            "source": processed_doc.source,
            "total_pages": processed_doc.total_pages,
            "text_chunks_saved": len(text_docs)
        }
    
    def add_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict]] = None
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì¶”ê°€
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            metadatas: ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶”ê°€ëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
        """
        return self.text_store.add_texts(texts, metadatas=metadatas)
    
    def search_text(self, query: str, k: int = 5) -> List[Document]:
        """
        í…ìŠ¤íŠ¸ ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        return self.text_store.similarity_search(query, k=k)
    
    # ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    def search_images(self, query: str, k: int = 5) -> List[Document]:
        """ë ˆê±°ì‹œ í˜¸í™˜ì„±ìš© - search_textì™€ ë™ì¼"""
        return self.search_text(query, k=k)
    
    def search_all(self, query: str, k: int = 5) -> Dict[str, List[Document]]:
        """
        ê²€ìƒ‰ (ë ˆê±°ì‹œ í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            {"text_results": [...], "image_results": [...]}
        """
        results = self.search_text(query, k=k)
        return {
            "text_results": results,
            "image_results": []  # ë” ì´ìƒ ì´ë¯¸ì§€ ê²°ê³¼ ì—†ìŒ
        }
    
    def search_with_scores(
        self,
        query: str,
        k: int = 5,
        score_threshold: Optional[float] = None
    ) -> List[tuple]:
        """
        ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            score_threshold: ì ìˆ˜ ì„ê³„ê°’ (ì´í•˜ë§Œ ë°˜í™˜)
            
        Returns:
            (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        results = self.text_store.similarity_search_with_score(query, k=k)
        
        if score_threshold is not None:
            results = [(doc, score) for doc, score in results if score <= score_threshold]
        
        return results
    
    def get_by_source(self, source: str) -> Dict:
        """
        ì†ŒìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ì¡°íšŒ
        
        Args:
            source: ì†ŒìŠ¤ ê²½ë¡œ/ì´ë¦„
            
        Returns:
            ì¡°íšŒ ê²°ê³¼
        """
        text_results = self.text_store.get(where={"source": source})
        return {"text_documents": text_results}
    
    def delete_by_source(self, source: str) -> bool:
        """
        ì†ŒìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ì‚­ì œ
        
        Args:
            source: ì†ŒìŠ¤ ê²½ë¡œ/ì´ë¦„
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            text_docs = self.text_store.get(where={"source": source})
            if text_docs and text_docs['ids']:
                self.text_store.delete(ids=text_docs['ids'])
            return True
        except Exception as e:
            print(f"âŒ ì‚­ì œ ì˜¤ë¥˜: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """ì €ì¥ì†Œ í†µê³„ ë°˜í™˜"""
        return {
            "text_store_count": self.text_store._collection.count(),
            "persist_dir": self.persist_dir
        }
